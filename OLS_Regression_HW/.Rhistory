# Prints the results of the Jarque-Bera Test to assess whether residuals are normal (package: tseries)
jarque.bera.test(ols_reg$residuals)
# Compute standardized residuals from the OLS regression which makes residuals comparable across observations
standardised_res<-rstandard(ols_reg)
# For each observation, calculate the mean standardized residual of its neighbors to identify whether high or low residuals cluster spatially
resnb<-sapply(queen_neighbors, function(x) mean(standardised_res[x]))
regress_data$standardised_res <- standardised_res    #creating a new variable in the shapefile shp.
OLS.Residuals.Map <- tm_shape(regress_data)+
tm_fill(col='standardised_res',
style='quantile',
title='Standardized OLS Residuals',
palette ='Blues')+
tm_borders(col = "gray", lwd = 0.5) +  # Add this line
tm_layout(frame=FALSE, title='Standardized OLS Residuals')
OLS.Residuals.Map
#tmap_save(OLS.Residuals.Map, filename = "HW2_Plots/OLS_Residuals_Map.png", width = 8, height = 8, dpi = 300)
#Regressing residuals on their nearest neighbors.
regress_res_lm <- lm(formula=standardised_res ~ resnb)
summary(regress_res_lm)
# Perform a Monte Carlo test for Moran’s I on the standardized OLS residuals
# Arguments:
#  - standardised_res: residuals from the OLS model, standardized for comparability
#  - queenlist: the spatial weights list (queen contiguity)
#  - 999: number of random permutations (higher = more precise p-value)
#  - alternative="two.sided": tests both positive and negative spatial autocorrelation
moran.mc(standardised_res, queenlist, 999, alternative="two.sided")
# The plot shows the relationship between each observation’s residual and the average residuals of its neighboring observations
# Arguments:
#  - standardised_res: residuals from the OLS model, standardized for comparability
#  - queenlist: the spatial weights list (queen contiguity)
moran.plot(standardised_res, queenlist)
# Dependent variable: LNMEDINC (log of median income)
# Independent variables: LNMEDHVAL (log of median home value), PCTVACANT (percent vacant housing)
# Spatial weights: queenlist (based on queen contiguity)
lag_reg<-lagsarlm(formula=LNMEDHVAL ~ LNNBELPOV + PCTBACHMOR + PCTSINGLES + PCTVACANT, data=regress_data, queenlist)
summary(lag_reg)
LR.Sarlm(lag_reg, ols_reg) #Here lag_reg is the SL output; ols_reg is the OLS output
#Prints the results of the Breusch-Pagan Test to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(lag_reg, studentize=FALSE)
#Prints the results of the Koenker-Bassett Test (also known as the Studentized Breusch-Pagan Test) to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(lag_reg)
#Prints the results of the Jarque-Bera Test to assess whether residuals are normal (package: tseries)
jarque.bera.test(lag_reg$residuals)
lag_res <- lag_reg$residuals
# This checks whether any spatial autocorrelation remains after accounting for the spatial lag
# Arguments:
#  - res_lag: residuals from the spatial lag model
#  - queenlist: spatial weights list (queen contiguity)
#  - 999: number of random permutations for the Monte Carlo simulation
#  - alternative="two.sided": tests both positive and negative spatial autocorrelation
lag_moran_mc <- moran.mc(lag_res, queenlist,999, alternative="two.sided")
lag_moran_mc
# Lag residual plot
moran.plot(lag_res, queenlist)
error_reg <- errorsarlm(formula=LNMEDHVAL ~ LNNBELPOV + PCTBACHMOR + PCTSINGLES + PCTVACANT, data=regress_data, queenlist)
# Extract residuals from the fitted spatial error model
error_res <- residuals(error_reg)
# Compute the mean residual value for each observation's neighbors
error_res_mean <- sapply(queen_neighbors, function(x) mean(error_res[x]))
summary(error_reg)
# Perform a likelihood ratio (LR) test comparing the spatial error model to OLS
#  - error_reg: fitted spatial error model (errorsarlm)
#  - ols_reg: fitted OLS model
LR.Sarlm(error_reg, ols_reg)
#Prints the results of the Breusch-Pagan Test to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(error_reg, studentize=FALSE)
#Prints the results of the Koenker-Bassett Test (also known as the Studentized Breusch-Pagan Test) to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(error_reg)
#Prints the results of the Jarque-Bera Test to assess whether residuals are normal (package: tseries)
jarque.bera.test(error_reg$residuals)
# Perform a Monte Carlo test for Moran's I on the residuals of the spatial error model
#  - error_res: residuals from the spatial error model (errorsarlm)
#  - queenlist: spatial weights list (queen contiguity)
#  - 999: number of random permutations for the Monte Carlo simulation
#  - alternative="two.sided": tests for both positive and negative spatial autocorrelation
error_moran_mc<-moran.mc(error_res, queenlist, 999, alternative="two.sided")
error_moran_mc
# Error residual plot
moran.plot(error_res, queenlist)
# convert regress_data from df to spatial data
regress_spatial <- as(regress_data, 'Spatial')  #These analyses are easier to do when the data are of the SpatialPolygonsDataFrame class
class (regress_spatial)
# Select an adaptive bandwidth for Geographically Weighted Regression (GWR)
#   - formula: dependent variable (LNMEDINC) and predictors (LNMEDHVAL, PCTVACANT)
#   - data: regress_spatial must be a SpatialPolygonsDataFrame (converted from sf)
#   - method = "aic": uses Akaike Information Criterion to select the optimal bandwidth
#   - adapt = TRUE: uses an adaptive bandwidth (proportion of nearest neighbors)
#     instead of a fixed distance bandwidth
bandwidth <- gwr.sel(formula = LNMEDHVAL ~ LNNBELPOV + PCTBACHMOR + PCTSINGLES + PCTVACANT,
data = regress_spatial,
method = "aic",
adapt = TRUE)
bandwidth
#setting a fixed bandwidth
bandwidth_fixed<-gwr.sel(formula=LNMEDHVAL ~ LNNBELPOV + PCTBACHMOR + PCTSINGLES + PCTVACANT,
data=regress_spatial,
method = "aic",
adapt = FALSE)
bandwidth_fixed
# Formula: dependent variable (LNMEDINC) and predictors (LNMEDHVAL, PCTVACANT)
#   - data: regress_data (SpatialPolygonsDataFrame)
#   - adapt = bandwidth: uses the adaptive bandwidth selected earlier (proportion of nearest neighbors)
#   - gweight = gwr.Gauss: applies a Gaussian weighting kernel to give nearer observations more influence
#   - se.fit = TRUE: returns local standard errors for each coefficient estimate
#   - hatmatrix = TRUE: stores diagnostic info for model evaluation (e.g., local R²)
gwr_model<-gwr(formula=LNMEDHVAL ~ LNNBELPOV + PCTBACHMOR + PCTSINGLES + PCTVACANT,
data=regress_spatial,
adapt = bandwidth, #adaptive bandwidth determined by proportion of observations accounted for
gweight=gwr.Gauss,
se.fit=TRUE, #to return local standard errors
hatmatrix = TRUE)
gwr_model
# GWR adaptive moran's i
gwr_adaptive_residuals <- gwr_model$SDF$gwr.e
gwr_moran_mc <- moran.mc(gwr_adaptive_residuals, queenlist, nsim = 999)
gwr_moran_mc
# GWR adaptive moran's i plot
gwr_adaptive_plot <- moran.plot(gwr_adaptive_residuals, queenlist)
# results using fixed bandwidth
gwr_model_fixed<-gwr(formula=LNMEDHVAL ~ LNNBELPOV + PCTBACHMOR + PCTSINGLES + PCTVACANT,
data=regress_spatial,
bandwidth = bandwidth_fixed, #fixed bandwidth
gweight=gwr.Gauss,
se.fit=TRUE, #to return local standard errors
hatmatrix = TRUE)
gwr_model_fixed
# GWR adaptive moran's i
gwr_fixed_residuals <- gwr_model_fixed$SDF$gwr.e
gwr_moran_mc <- moran.mc(gwr_fixed_residuals, queenlist, nsim = 999)
gwr_moran_mc
# GWR adaptive moran's i plot
gwr_adaptive_plot <- moran.plot(gwr_fixed_residuals, queenlist)
# Summarize the SpatialDataFrame output from the GWR model
summary(gwr_model$SDF)
gwr_results <- as.data.frame(gwr_model$SDF)
# LNNBELPOV
regress_data$coefLNNBELPOVst <- gwr_results$LNNBELPOV/gwr_results$LNNBELPOV_se
# PCTBACHMOR
regress_data$coefPCTBACHMORst <- gwr_results$PCTBACHMOR/gwr_results$PCTBACHMOR_se
# PCTSINGLES
regress_data$coefPCTSINGLESst <- gwr_results$PCTSINGLES/gwr_results$PCTSINGLES_se
# PCTVACANT
regress_data$coefPCTVACANTst <- gwr_results$PCTVACANT/gwr_results$PCTVACANT_se
regress_data$gwrE <- gwr_results$gwr.e
regress_data$localR2 <- gwr_results$localR2
gwr_results
#The maps above are the ones using the exact code that our professor had but it looked funky so here is the code that I made to try and fix it.
library(tmap)
# LNNBELPOV
coefLNNBELPOV <- tm_shape(regress_data) +
tm_fill(
col = "coefLNNBELPOVst",
breaks = c(-Inf, -6, -4, -2, 0, 2, 4, 6, Inf),
title = "Standardized Coef: LNNBELPOV",
palette = "-RdBu"
) +
tm_borders(col = "gray", lwd = 0.5) +
tm_layout(
frame = FALSE,
#title = "Percent of Number in Poverty (Log)",
title.position = c("center", "top"),
title.size = 1.2,
legend.position = c("RIGHT", "BOTTOM"),
legend.just = c("right", "bottom"),
legend.bg.color = "white",
legend.bg.alpha = 0.9,
legend.text.size = 0.6,
legend.title.size = 0.6,
inner.margins = c(0.1, 0.15, 0.1, 0.15)
)
coefLNNBELPOV
#tmap_save(coefLNNBELPOV, filename = "HW2_Plots/coefLNNBELPOV_Map.png", width = 8, height = 8, dpi = 300)
# PCTBACHMOR
coefPCTBACHMOR <- tm_shape(regress_data) +
tm_fill(
col = "coefPCTBACHMORst",
breaks = c(-Inf, -6, -4, -2, 0, 2, 4, 6, Inf),
title = "Standardized Coef: PCTBACHMOR",
palette = "-RdBu"
) +
tm_borders(col = "gray", lwd = 0.5) +
tm_layout(
frame = FALSE,
#title = "Percent of Bachelor's or More",
title.position = c("center", "top"),
title.size = 1.2,
legend.position = c("RIGHT", "BOTTOM"),
legend.just = c("right", "bottom"),
legend.bg.color = "white",
legend.bg.alpha = 0.9,
legend.text.size = 0.6,
legend.title.size = 0.6,
inner.margins = c(0.1, 0.15, 0.1, 0.15)
)
coefPCTBACHMOR
#tmap_save(coefPCTBACHMOR, filename = "HW2_Plots/coefPCTBACHMOR_Map.png", width = 8, height = 8, dpi = 300)
coefPCTSINGLES <- tm_shape(regress_data) +
tm_fill(
col = "coefPCTSINGLESst",
breaks = c(-Inf, -6, -4, -2, 0, 2, 4, 6, Inf),
title = "Standardized Coef: PCTSINGLES",
palette = "-RdBu"
) +
tm_borders(col = "gray", lwd = 0.5) +
tm_layout(
frame = FALSE,
#title = "Percent of Detached Single Homes",
title.position = c("center", "top"),
title.size = 1.2,
legend.position = c("RIGHT", "BOTTOM"),
legend.just = c("right", "bottom"),
legend.bg.color = "white",
legend.bg.alpha = 0.9,
legend.text.size = 0.6,
legend.title.size = 0.6,
inner.margins = c(0.1, 0.15, 0.1, 0.15)
)
coefPCTSINGLES
#tmap_save(coefPCTSINGLES, filename = "HW2_Plots/coefPCTSINGLES_Map.png", width = 8, height = 8, dpi = 300)
# PCTVACANT
coefPCTVACANT <- tm_shape(regress_data) +
tm_fill(
col = "coefPCTVACANTst",
breaks = c(-Inf, -6, -4, -2, 0, 2, 4, 6, Inf),
title = "Standardized Coef: PCTVACANT",
palette = "-RdBu"
) +
tm_borders(col = "gray", lwd = 0.5) +
tm_layout(
frame = FALSE,
#title = "Percent of Vacancies",
title.position = c("center", "top"),
title.size = 1.2,
legend.position = c("RIGHT", "BOTTOM"),
legend.just = c("right", "bottom"),
legend.bg.color = "white",
legend.bg.alpha = 0.9,
legend.text.size = 0.6,
legend.title.size = 0.6,
inner.margins = c(0.1, 0.15, 0.1, 0.15)
)
coefPCTVACANT
#tmap_save(coefPCTVACANT, filename = "HW2_Plots/coefPCTVACANT_Map.png", width = 8, height = 8, dpi = 300)
arranged_coef <- tmap_arrange(coefLNNBELPOV, coefPCTBACHMOR, coefPCTSINGLES, coefPCTVACANT, ncol = 4)
arranged_coef
tmap_save(arranged_coef, filename = "HW2_Plots/arranged_coef_Map_columns.png", width = 8, height = 8, dpi = 300)
map <- tm_shape(regress_data)+
tm_fill(col='localR2',
breaks=c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7),
n=5,
palette = 'Blues',
title = expression(Local~R^2)) +  # Add custom legend title
tm_borders(col = "white", lwd = 0.5) +
tm_layout(frame=FALSE)
map
# Save it
#tmap_save(map, filename = "HW2_Plots/LocalR2_Map.png", width = 8, height = 8, dpi = 300)
# Compute Moran's I of residuals for each model to measures remaining spatial autocorrelation.
# The closer the Moran's I statistic is to 0, the better.
moran_ols <- moran.mc(standardised_res, queenlist, 999, alternative = "two.sided")
moran_lag <- moran.mc(lag_reg$residuals, queenlist, 999, alternative = "two.sided")
moran_error <- moran.mc(error_reg$residuals, queenlist, 999, alternative = "two.sided")
moran_gwr <- moran.mc(gwr_model$SDF$gwr.e, queenlist, 999, alternative = "two.sided")
moran_ols
moran_lag
moran_error
moran_gwr
# Compare AIC values.
# The lower the AIC, the better the model fit (after penalization).
aic_ols <- AIC(ols_reg)
aic_lag <- AIC(lag_reg)
aic_error <- AIC(error_reg)
aic_gwr <- gwr_model$results$AICc   # GWR reports a corrected AIC (AICc)
aic_ols
aic_lag
aic_error
aic_gwr
# Compare Log-Likelihoods and Likelihood Ratio Tests.
# Higher log-likelihood = better model fit.
loglik_ols   <- logLik(ols_reg)
loglik_lag   <- logLik(lag_reg)
loglik_error <- logLik(error_reg)
# Likelihood ratio tests
lr_lag_vs_ols   <- LR.Sarlm(lag_reg, ols_reg)
lr_error_vs_ols <- LR.Sarlm(error_reg, ols_reg)
lr_lag_vs_ols
lr_error_vs_ols
# Significant LR test (p < 0.05) = spatial model significantly improves on OLS.
# Compare R-squared values (OLS vs. GWR) GWR = quasi-global R-squared to compare with OLS.
r2_ols <- summary(ols_reg)$r.squared
# OLS comes with the R-squared value, but gwr does not. We need to calculate the R-squared value
y <- regress_data$LNMEDHVAL # Dependent variable vector
r2_gwr <- 1 - (gwr_model$results$rss / sum((y - mean(y))^2))
r2_ols
r2_gwr
# Higher R² = model explains more variance.
# Comparison summary table
comparison <- data.frame(
Model = c("OLS", "Spatial Lag", "Spatial Error", "GWR (Adaptive)"),
# AIC: Lower AIC indicates better model fit after penalization
AIC = c(
AIC(ols_reg),
AIC(lag_reg),
AIC(error_reg),
gwr_model$results$AICc   # Use AICc from GWR
),
# Log-likelihood: Higher value indicates better fit
LogLik = c(
as.numeric(logLik(ols_reg)),
as.numeric(logLik(lag_reg)),
as.numeric(logLik(error_reg)),
NA   # LogLik not reported for GWR
),
# R-squared: Proportion of variance explained
R2 = c(
summary(ols_reg)$r.squared,
NA,  # Not reported for SAR/SEM
NA,  # Not reported for SEM
r2_gwr  # Quasi-global R2
)
)
comparison
data.frame(
Model = c("Spatial Lag", "Spatial Error"),
LogLik = c(logLik(lag_reg), logLik(error_reg)),
AIC = c(AIC(lag_reg), AIC(error_reg)),
SC = c(BIC(lag_reg), BIC(error_reg))
)
data.frame(
Model = c("OLS", "Spatial Lag", "Spatial Error"),
LogLik = c(logLik(ols_reg), logLik(lag_reg), logLik(error_reg)),
AIC = c(AIC(ols_reg), AIC(lag_reg), AIC(error_reg)),
SC = c(BIC(ols_reg), BIC(lag_reg), BIC(error_reg))
)
# Formula: dependent variable (LNMEDINC) and predictors (LNMEDHVAL, PCTVACANT)
#   - data: regress_data (SpatialPolygonsDataFrame)
#   - adapt = bandwidth: uses the adaptive bandwidth selected earlier (proportion of nearest neighbors)
#   - gweight = gwr.Gauss: applies a Gaussian weighting kernel to give nearer observations more influence
#   - se.fit = TRUE: returns local standard errors for each coefficient estimate
#   - hatmatrix = TRUE: stores diagnostic info for model evaluation (e.g., local R²)
gwr_model<-gwr(formula=LNMEDHVAL ~ LNNBELPOV + PCTBACHMOR + PCTSINGLES + PCTVACANT,
data=regress_spatial,
adapt = bandwidth, #adaptive bandwidth determined by proportion of observations accounted for
gweight=gwr.Gauss,
se.fit=TRUE, #to return local standard errors
hatmatrix = TRUE)
gwr_model
# results using fixed bandwidth
gwr_model_fixed<-gwr(formula=LNMEDHVAL ~ LNNBELPOV + PCTBACHMOR + PCTSINGLES + PCTVACANT,
data=regress_spatial,
bandwidth = bandwidth_fixed, #fixed bandwidth
gweight=gwr.Gauss,
se.fit=TRUE, #to return local standard errors
hatmatrix = TRUE)
gwr_model_fixed
#| eval: false
#| echo: true
library(sf)
library(dplyr)
library(tibble)
library(spdep)
library(spgwr)
library(tmap)
library(spatialreg)
library(whitestrap)
library(lmtest)
library(tseries)
options(scipen=999)
#Read the Regression Data shp
regress_data <- st_read("data/RegressionData.shp")
#Create variable PLUS1, defined as NBELPOV100+1
#We already have LNNBELPOV and LNMEDHVAL in the shapefile (1.a.iv, 1.a.v in hw instructions)
regress_data <- regress_data %>%
mutate(
PLUS1 = NBelPov100 + 1,
)
#Before proceeding, let’s examine the distributions of our variables and see whether we need to create log transformations of any of them
par(oma=c(0,0,2,0))
par(mfrow=c(1,3))
hist(regress_data$MEDHHINC, breaks = 50)
hist(regress_data$MEDHVAL, breaks = 50)
hist(regress_data$PCTVACANT, breaks = 50)
#PCTVANCANT has a spike at 0, so not transformation for them. MEDHHINC and MEDHVAL will be transformed/they already exist in the regress_data gdf.
#Graph of LNMED(HH)INC, LNMEDHVAL, and PCTVACANT
par(oma=c(0,0,2,0))
par(mfrow=c(1,3))
hist(regress_data$LNMEDINC, breaks = 50)
hist(regress_data$LNMEDHVAL, breaks = 50)
hist(regress_data$PCTVACANT, breaks = 50)
#I am making an assumption that LNMEDINC in the log of MEDHHINC. To reiterate, the LNMEDINC and LNMEDVHAL are present when you load the regress_data shp.
#defining neighbors for each of the block groups in philly using queen neighbors
queen_neighbors <- poly2nb(regress_data, row.names = regress_data$POLY_ID)
summary(queen_neighbors)
#It is useful to examine where block groups with non-average neighbor patterns are situated before we run spatial analyses. This is because these outlier block groups will affect our spatial analyses. We will look at the block groups where there is only 1 neighbor, and block groups where there are 27 neighbors. The block groups themselves will be colored in red, and their neighbors in green
#see which region has only one neighbor
smallestnbcard<-card(queen_neighbors) #extract neighbor matrix
smallestnb<-which(smallestnbcard == min(smallestnbcard)) #extract block groups with smallest number of neighbors
fg<-rep('grey90', length(smallestnbcard))
fg[smallestnb]<-'red' #color block groups red
fg[queen_neighbors[[smallestnb[1]]]]<-'green' #color neighboring blocks green
fg[queen_neighbors[[smallestnb[2]]]]<-'green'
fg[queen_neighbors[[smallestnb[3]]]]<-'green'
fg[queen_neighbors[[smallestnb[4]]]]<-'green'
plot(regress_data$geometry, col=fg)
title(main='Regions with only 1 neighbor')
#block group with most neighbors
largestnbcard<-card(queen_neighbors)
largestnb<-which(largestnbcard == max(largestnbcard))
fg1<-rep('grey90', length(largestnbcard))
fg1[largestnb]<-'red'
fg1[queen_neighbors[[largestnb]]]<-'green'
plot(regress_data$geometry, col=fg1)
title(main='Region with 27 neighbors')
#Choosing W as the style parameter, which will perform row standardization.
queenlist <- nb2listw(queen_neighbors, style = "W")
moran(regress_data$LNMEDHVAL, queenlist, n=length(queenlist$neighbours), S0 =Szero(queenlist))$"I"
# Check if this measure is statistically significant
moranI_mc <- moran.mc(regress_data$LNMEDHVAL, queenlist, nsim=999, alternative="two.sided")  #We use 999 permutations
moranI_mc
# Histogram of the Moran’s I values from random permutations
moranI_mc_res<-moranI_mc$res
hist(moranI_mc_res, freq=10000000, nclass=100)   #Draws distribution of Moran's I's calculated from randomly permuted values
# Here, we draw a red vertical line at the observed value of our Moran's I
abline(v=moran(regress_data$LNMEDHVAL, queenlist, n=length(queenlist$neighbours), S0=Szero(queenlist))$`I`, col='red')
#Create Moran plot (lagged value against observed value)
moran.plot(regress_data$LNMEDHVAL, queenlist)
#Run local moran's I (LISA)
lmoran<-localmoran(regress_data$LNMEDHVAL, queenlist)
head(lmoran)
# Combine the original regression dataset with the Local Moran's I results so that each observation includes its corresponding local spatial statistic
lmoran_df <-cbind(regress_data, as.data.frame(lmoran))
library(tmap)
tmap_mode("plot")
#Obtaining the Local Moran's P-Values (two-sided)
regress_data$local_I <- lmoran[, "Pr(z != E(Ii))"]
library(sf)
regress_data <- st_make_valid(regress_data) #Sometimes necessary if projection is off
#Creating the LISA Clusters
moran_plot <- moran.plot(as.vector(scale(regress_data$LNMEDHVAL)), queenlist)
regress_data$quadrant <- NA
# high-high
regress_data[(moran_plot$x >= 0 & moran_plot$wx >= 0) & (regress_data$local_I <= 0.05), "quadrant"]<- 1
# low-low
regress_data[(moran_plot$x <= 0 & moran_plot$wx <= 0) & (regress_data$local_I <= 0.05), "quadrant"]<- 2
# high-low
regress_data[(moran_plot$x >= 0 & moran_plot$wx <= 0) & (regress_data$local_I <= 0.05), "quadrant"]<- 3
# low-high
regress_data[(moran_plot$x <= 0 & moran_plot$wx >= 0) & (regress_data$local_I <= 0.05), "quadrant"]<- 4
# non-significant
regress_data[(regress_data$local_I > 0.05), "quadrant"] <- 5
# LISA P-Value Map
p_vals <- tm_shape(regress_data) +
tm_polygons(col = "local_I", title = "",
breaks = c(-Inf, 0.001, 0.01, 0.05, Inf),
palette = c("darkblue", "blue", "lightblue", "white")) +
tm_borders(col = "gray", lwd = 0.5) +
tm_layout(
legend.outside = TRUE,
legend.text.size = 1,
legend.title.size = 1,
fontfamily = "Arial",
title = "LISA P-Value Map",
title.size = 1.2,
frame = FALSE
)
p_vals
#tmap_save(p_vals, filename = "HW2_Plots/p_vals_Map.png", width = 8, height = 8, dpi = 300)
# LISA Cluster Map
lisa_clusters <- tm_shape(regress_data) +
tm_fill(col = "quadrant", title = "",
breaks = c(1, 2, 3, 4, 5, 6),
palette = c("red", "blue", "lightpink", "skyblue2", "white"),
labels = c("High-High", "Low-Low", "High-Low", "Low-High", "Non-significant")) +
tm_borders(alpha = 0.5) +
tm_borders(col = "gray", lwd = 0.5) +
tm_layout(
frame = FALSE,
legend.outside = TRUE,
legend.text.size = 1,
legend.title.size = 1,
fontfamily = "Arial",
title = "LISA Cluster Map",
title.size = 1.2
)
lisa_clusters
#tmap_save(lisa_clusters, filename = "HW2_Plots/lisa_clusters_Map.png", width = 8, height = 8, dpi = 300)
# Fit an Ordinary Least Squares (OLS) regression model
# Dependent variable: LNMEDINC (log of median income)
# Independent variables: LNMEDHVAL (log of median home value) and PCTVACANT (percent of vacant housing)
ols_reg<-lm(formula=LNMEDHVAL ~ LNNBELPOV + PCTBACHMOR + PCTSINGLES + PCTVACANT, data=regress_data)
summary(ols_reg)
# OLS Residual vs. WT Residual
regress_data$OLS_RESIDU <- residuals(ols_reg)
regress_data$WT_RESIDU <- lag.listw(queenlist, regress_data$OLS_RESIDU)
moran_RESIDU <- moran.plot(regress_data$WT_RESIDU, queenlist)
moran_RESIDU <- moran.plot(regress_data$OLS_RESIDU, queenlist)
plot(regress_data$WT_RESIDU, regress_data$OLS_RESIDU)
# OLS Residual vs. WT Residual
regress_data$OLS_RESIDU <- residuals(ols_reg)
regress_data$WT_RESIDU <- lag.listw(queenlist, regress_data$OLS_RESIDU)
moran_RESIDU <- moran.plot(regress_data$WT_RESIDU, queenlist)
# OLS Residual vs. WT Residual
regress_data$OLS_RESIDU <- residuals(ols_reg)
regress_data$WT_RESIDU <- lag.listw(queenlist, regress_data$OLS_RESIDU)
moran_RESIDU <- moran.plot(regress_data$OLS_RESIDU, queenlist)
# OLS Residual vs. WT Residual
regress_data$OLS_RESIDU <- residuals(ols_reg)
regress_data$WT_RESIDU <- lag.listw(queenlist, regress_data$OLS_RESIDU)
moran_RESIDU <- moran.plot(regress_data$WT_RESIDU, queenlist)
moran_RESIDU <- moran.plot(regress_data$OLS_RESIDU, queenlist)
WT
moran_RESIDU <- moran.plot(regress_data$WT_RESIDU, queenlist)
